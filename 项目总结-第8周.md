# 应用features预处理
    在chemleon和squirrel数据集失败
为了探究原因，我提出了假说。

1. 随着邻接矩阵不断叠加，考虑的邻域范围扩大，导致相邻边的特征越来越接近。对于异行比较大的数据集来说，会产生一定的干扰。具体来说，**只有features本身和dot(adj,features)是重要的**
2. 由于我们的模型本身自带考虑了一层的邻居，所以事实上，只有features本身是有意义的，其余的干扰很强。

在cora上差不多
84.6278, 0.85
# 两种GCN
## 现象
有两种的GCN，一种是输入`F`,然后$H=A W F$作为encoder，一种是$H=W(AF)$相当于feature预处理之后，之后mlp链接。两种方式在cora数据集上表现相似，但在chameleon数据集上分别正确率在50(标准GCN)，72(预处理+Mlp)

## 分析

这是由于，在标准GCN中对F后乘作用于adj，在梯度传递的时候，有边的两个节点的特征会按照相同的方向移动。导致在降维过程中，有边的节点的低度数特征变得非常接近。在cora或者cite这样的边相同的节点，我们这么做是没有问题的。但是在cham或者squirrel这样的的数据集，会引入很大的噪声。而预处理+MLp的工作依靠的假设是，我们可以根据节点的自身特征和节点的邻域特征直接判断出节点的类别。

而我们的double_attention模型。详细公式为

1. Hidden=F*W
2. Neigbors=Hidden[neighbor_list]
3. ->neighbor_attention()->attention()

降维这一步的特征因为太浅，在求梯度的时候变得很小。而且在计算attention(i)，是使用的自己的独一的一套邻居，这导致在梯度传递的时候，使得有边的节点低纬表征相同的梯度被第一次的neigborattention稀释，也就是neigbor_attention的结果，提取出来的仅仅属于每一个节点的neighbors会变得越来越和自己相似。

## 总结
我们的attention在聪明的从邻居节点吸取特征这一点并没有学到很好，而是聪明的避免邻居节点过于相似。所以我得出的结论是，目前我们的模型在现有体系在
