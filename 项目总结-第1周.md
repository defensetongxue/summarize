1. 已经得到GAT的attention acore matric，并转化成邻接表的形式
2. 找到未分类成功的集合
3. 分析GAT的矩阵
   ```python
   example: 0,Wrong Point is 506,label is 4
   (506,506, att_score=0.5, 506 labels is 4)
   (506,514, att_score=0.5, 514 labels is 6)


   example: 1,Wrong Point is 509,label is 5
   (509,314, att_score=0.32, 314 labels is 5)
   (509,509, att_score=0.33, 509 labels is 5)
   (509,669, att_score=0.35, 669 labels is 5)


   example: 2,Wrong Point is 510,label is 3
   (510,192, att_score=0.2, 192 labels is 2)
   (510,356, att_score=0.22, 356 labels is 2)
   (510,510, att_score=0.2, 510 labels is 3)
   (510,519, att_score=0.19, 519 labels is 2)
   (510,2223, att_score=0.19, 2223 labels is 2)

   ```
   发现attention score接近于平均，对原因进行探索：发现从向量很难观察出区别（这是显然的）

   
   另外，我对正确的点位进行了相同的分析。发现，正确的点位相比于错误的点位，其邻居的自己以外的类占比很少。但是，也有例外的现象。
   ```python
   example: 11,right Point is 1203,label is 2
   (1203,542, att_score=0.1106, 542 labels is 2)
   (1203,779, att_score=0.1173, 779 labels is 2)
   (1203,816, att_score=0.1059, 816 labels is 2)
   (1203,881, att_score=0.1083, 881 labels is 2)
   (1203,1119, att_score=0.1085, 1119 labels is 2)
   (1203,1195, att_score=0.1153, 1195 labels is 2)
   (1203,1203, att_score=0.1083, 1203 labels is 2)
   (1203,1411, att_score=0.1075, 1411 labels is 2)
   (1203,1630, att_score=0.1183, 1630 labels is 2)

   right Point is 782,label is 2
   (782,782, att_score=0.4763, 782 labels is 2)
   (782,1532, att_score=0.5237, 1532 labels is 3)
   ```

   然后我对attentionscore对于度数的倒数进行差值，把这个差值平方后求平均，得到如下结论：
   ```python
   wrong_set 0.0004756846174132079, right set 0.0003090475802309811
   ```
   综上分析得到了结论**在attention score 即使经历了softmax上，也没有很大的区别**

   我对此的解释是，首先由于在embeding的过程中，仅仅把单词embeding出现的频率作为词向量损失了大量的特征，这导致不同类别的文章本身有一定难区分性。其次**匮乏的信息难以计算出有效的attentionscore**
   
   另外，节点的attention_score中正确和错误节点进行了简单的统计分析。
   ```python
      平均一个点有多少跨域引用点 wrong set 0.9097487926483154, right set 0.6403751373291016
      跨域引用点占据邻居的比例: wrong set 0.42795389048991356, right set 0.10374702187567685
      平均每个点跨域引用点的比例: wrong set 0.3596670619847353, right set 0.08931558355874128
   ```
   从边的数量来说，错误节点的平均边的数量页率低于引用点。
   ```python 
   平均每个节点拥有的边的数量:wrong_set 3.5359477124183005, right set 4.451003541912633
   ```

   得到的结论是**判断错误的节点有：边相对少，且跨域引用（干扰点）显著较多的**

   于是，我做了如下尝试**增大了计算attention score时候，激活函数leakyrelu在负方向的斜率，得到了部分的提高，但是由于这是选取了最好的模型，但实际使用中因为过拟合，最后的正确率并没有显著的提高，Test set results: loss= 0.6529 accuracy= 0.8450**

   **调整attention计算方式，用q，k，v三个矩阵代替原有的a矩阵，这个跑出来效果不理想（低于acc0.5），有可能是bug问题，正在排除**，

   剩余还没来得及尝试的想法是：attention矩阵会经过softmax，或许可以换用更敏感的方式。针对于度数的到处附近的部分有一个增强。

   可以尝试解决过拟合问题。
。
1. 还需要对对ADSF进行类似的工作，由于ADSF刚刚出来，实现的人不过，论文的附加的代码逻辑不清晰加上有比较大的数据缺失，它所依赖的几个pkl问价没有给出，虽然有人提出issue但是并没有得到回复。这一部分我的建议是仿照源码实现一个

从大方向上我有两个想法，

我认为直接把hidden按照比例相加起来，感觉不太妥当。因为cv或者nlp的向量都没有这么长。越长的向量在进行简单加法的时候会损失更多的信息。或许可以考虑把周边的信息相加之后和原向量拼接。

我觉得ADSF，对图形的架构利用其实是加大了输入神经网络的信息。这个图形结构有没有可能直接输入模型而不是作为参数。

从人物角度来说，或许可以做预处理。把近义词合并用更聪明的方式降维。

我的问题是，正常的attention大概是在什么范围的。

我还看到了一篇用nlp模型直接跑cv的，就是把图片直接当做语言模型跑的。我想到如果用图算法跑cv，那么其实最后形成的图会变得很复杂，边会变得比较多。边越复杂对attention的优化就越大。有没有可能通过图结构进行潜在的加边，对数据增强。

换数据集？
Chan

ADSF度数，全图所有最短路径出现的此处

GATv2

deep work 无监督的节点embed

dropout edge ： 怎么加边 根据每个点改聚合
GEMO
graph sage 