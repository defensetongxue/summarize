## word2vec和gae在 在Graph中的使用

### 关注到的问题
1. 由于在图训练的数据集中，特征往往会是以稀疏矩阵的方式出现，导致在通过一层一层线性层后，在训练之处特征的区分度低。QKV的attention方式常常发生失效问题，GAT收到初始化的影响比较大。在训练初期，attention不能够找到什么节点的是重要的。因为稀疏矩阵在通过随机初始化的线性矩阵会变得难以区分。在之后的训练中，由于邻居和自身有一个相同的梯度，在训练集较少的时候，后续训练回趋向于局部最小值。
    注：设在原始features中，平均每个节点的featurs中，有值的部分占总features长度为稀疏度

| dataset   | averge-nonzero | features_len | %       |
| --------- | -------------- | ------------ | ------- |
| cora      | 18             | 1433         | 0.01268 |
| citeseer  | 31             | 3703         | 0.0085  |
| pubmed    | 50             | 500          | 0.10    |
| squirrel  | 18             | 2089         | 0.008   |
| chameleon | 12.8           | 2325         | 0.0055  | 

1. deepwalk的任务是“关系”挖掘，并且没有使用features作为原信息。GAE但是由于在运算过程中，采用了两层的gcn,导致过平滑。我们的目的是，能够在一个节点的邻域内找到重要程度更高的节点。所以gcn是不可取的。
解决方案，采取word2vec在features方面的应用和改进版本的gae初始化不同head的降维线性矩阵。

达成效果，

1. 考虑到features相似性的特征，比如有的节点有1，3位的feature，有的有2，3，那么可以认为这两个节点可能是同类节点，即使没有直接的边相连
2. 自适应的考虑不同程度的信息，用焦距不同特征的初始化来丰富多头attention
### 实验效果

F W =H

在cora，citeseer在初始化GAT-8头，

attention初始化方式为[gae,w2v,bc,random]*2

在标准分割上：10次                         
cora :    80.9500, 0.85 ->81.6400, 0.78     ->82.0000, 0.72

citseer : 67.18 1.41 ->69.6800, 0.91    

用embed代替features过单层的GCN 67.9100, 1.04

在Geom分割下，单层attention+分类器，QKV_attention，GATattention，在cora，citeseer上都有提高0.5-1.5%

但是在DGAT上不管用

### 类似的工作
deepwalk，node2vec，gae不采用这些已有方法的最根本原因是，不存在一个features到hid的线性降维矩阵

dropedge
### word2vec
目标：在features上进行降为，找到邻居中features和自身更为相似的的节点给以更高的attention取值

取一个节点的距离少于等于2的邻居构建词对。用skipgram模型，使一个节点预测其邻居的特征。用中间结果为embeding。

详细算法：
设 features=X，X.shap=(N,F)为原始feature初始化后的取值

W_1=降维矩阵 W_1.shape=(F,H),H是隐藏层，设置为64

W_2=生成矩阵 W_2.shape(H,F),

pred=$XW_1W_2$

loss=(neigbor_of_x,pred)
### GAE改
目标： 找的在图形上和自身更接近的节点作为更高的attention取值

思路，让节点的隐向量和与自己有相同的k-hop邻居更为相似

算法

设 features=X，X.shap=(N,F)为原始feature初始化后的取值

W_1=降维矩阵 W_1.shape=(F,H),H是隐藏层，设置为64

pred=$(XW_1)(XW_1).T$

loss=(k-hop-adj,pred)
### bc 
在features内部预训练，

用某一个节点的features，一位的features预测另一位的features

## DGAT story
### 注意到
1. GAT中的attention计算的全局的attention取值，GATv2对其进行了改进，计算了不同节点相对于中心节点的局部的attention。然而考虑一个节点的重要性，单单考虑其自身和中心节点是不足的。需要考虑在一个局部的相对attention
2. 与此同时，在遇到异性数据集的时候，节点会给自身的邻居一个相同的梯度，导致邻居和自身收到了相同的惩罚。这并不是合理的。于是，对于邻居的投票，我们采用更深一层的网络，来平滑labelde惩罚。使得在梯度传递的时候，此节点在中心节点中的镜像承接一部分的惩罚。

和GATv2十分类似，观测的结果是再简单的问题中，效果更差，在复杂的问题中效果更好

## 开销实在太长了

amazon-poto数据大小56G，一共1e9个节点
## motivation

## 

## 说明降维矩阵随机会导致attention趋同，做具体的实验

## 参数初始化，人工attention一开始不趋同，一开不趋同错误

## 考虑的信息，目前无监督区别在哪里

## 实验

## 9 acclear
## 8 tirbOl 
## 