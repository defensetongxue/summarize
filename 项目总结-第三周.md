
# 论文阅读记录
### GEom-GCN/原文
文章为了解决GCN存在的两个问题，提出来

1. 无法聚合距离较远的点
2. 没有考虑图形的几何信息

于是，论文提出按照graph embeding后的向量划分节点和节点之间的关系。具体来说，针对每一个节点$i$和本轮特征$h^i$，以及graph embeding后的向量ri，划分成四种关系（可以按照ri之间的距离）。每一种关系的节点聚合后拼接，通过非线性的单层神经网络生成下一次迭代的向量$h^{i+1})$

关于graph embeding，他提出了三种方式的尝试，并对应三种模型

Isomap Geom-GCN-I

Poincare  Geom-GCN-P 

struc2vec  Geom-GCN-S

本质上是用图形结构定义了新的邻接矩阵，发掘潜在的邻居

### GDC 图扩散网络
把邻接矩阵改成nhop矩阵，引入注意力机制。自适应为每一个节点学习n。

### Understanding Attention and Generalizationin Graph Neural Networks 原文

做了有关threshold的研究。让attention取值在摸一个额定取值之外的边被抛弃。或者每一层pooling之后都留下r=0.8的边进入下一次迭代。

发现attention的取值都很相近，导致大量的边被同时的舍弃和保留。

之后文章提出了自己的模型ChebyGin，模型按照度数的比例聚合了距离自身k内的节点，作为自身的特征。是一个GCN的扩展。

相比于GIN和GCN在colors（程序生成计算绿色节点数量的数据集），triangle（数三角形），以及MINST-75AP（image分类）数据集sota。


**idea：不采用threshold，而是按照排名去加边或者减少边，考虑到所有的节点而不是adj的节点。**
### GIN/转述
核心公式 

$h_v^{(k)}=MLP^{(k)}((1+\epsilon ^{(k)})h_v^{(k-1)}+\Sigma_{u\in N(v)}h^{(k-1)})$

在实际观察中，发现$\epsilon$固定为0的时候略微但始终优于可学习的$\epsilon$，GIN模型在相比于GCN，和GraphSAGE取得了sota。

证明了MLP在计算hi的时候会有帮助。



**idea：不采用threshold，而是按照排名去加边或者减少边，考虑到所有的节点而不是adj的节点。**
# 做过的尝试
把图形结构的嵌入结果 [N,node_number]维度的向量称为 ri

节点的本身特征称为hi

## 图形信息对模型效果的重要性
朴素GAT

Test set results: loss= 0.7370 accuracy= 0.8330 ( 86.37 没有成功做出这样的结果)

### 只用ri进行GAT训练
Test set results: loss= 0.9220 accuracy= 0.7730

### 把ri和hi拼接，输入GAT
$$GAT (hi || ri || degree)$$
Test set results: loss= 0.8344 accuracy= 0.7850
Loading 524th epoch r=0.8
Test set results: loss= 0.7832 accuracy= 0.8030
Loading 1025th epoch r=0.95
Test set results: loss= 0.6741 accuracy= 0.8370

### hi 和 ri 分开 让模型变得更简单

$$ei= LeakyReLu(Att1_{ij}(Wh_i)) ||  LeakyReLu(Att2_{ij}(Wr_i))$$

$$h_{i+1}=fc(ei)$$
Test set results: loss= 0.6311 accuracy= 0.8220
**ri应用attention计算会导致模型过于复杂，可以去掉attention**

## 聚合远距离节点
### 用attention的排名决定是否加边，确保attention排名在前k位的取值被保留。
k=1
Test set results: loss= 0.8828 accuracy= 0.8150
k=5
Test set results: loss= 1.2182 accuracy= 0.6780

**分析为什么用feaeture的attention会让结果更差，猜测是因为使用feature的attention其实只是找到和自身最相似的node进行聚合，和简单把自身倍增没有本质区别。应从节点的结构考虑进行加边**


**用图形结构的attention计算feature的聚合**
Loading 847th epoch cora
Test set results: loss= 0.7440 accuracy= 0.8220

Loading 874th epoch train_val_test

[0.2,0.2,0.6]

Test set results: loss= 0.8933 accuracy= 0.7734 (GAT 74 )

train_val_test [0.6,0.2,0.2]

Test set results: loss= 1.0252 accuracy= 0.7313
**改进ri的计算方式，改进rwr使得重启的时候等可能的到i和i的邻域里，或者考虑k-hop矩阵的时候，不仅仅考虑哪一个k-hop而是把k-hop矩阵进行堆叠**

# 遇到的问题 
在实际使用chameleon数据集的时候发现，数据的类别非常多，有的类只有几个点。geom应该是在原始数据做过预处理，如果要继续使用这个数据集，需要直接用geom预处理过数据。而且这个数据并没有发现除了geom以外的文章有过采用。

我认为扩充邻居是一个非常重要的方式，目前已有的文章的专注点大多集中在：优化attention计算方式(GEOM)，扩充邻居的边（graphic diffusion），或者两者结合（ADSF[在原有attention基础上加上structural—fingerprint的标量]）,

