1.可以考虑的思路 
验证GATv2，弄清楚GATv2

# 论文阅读记录
### GEom-GCN/原文
文章为了解决GCN存在的两个问题，提出来

1. 无法聚合距离较远的点
2. 没有考虑图形的几何信息

于是，论文提出按照graph embeding后的向量划分节点和节点之间的关系。具体来说，针对每一个节点$i$和本轮特征$h^i$，以及graph embeding后的向量ri，划分成四种关系（可以按照ri之间的距离）。每一种关系的节点聚合后拼接，通过非线性的单层神经网络生成下一次迭代的向量$h^{i+1})$

关于graph embeding，他提出了三种方式的尝试，并对应三种模型

Isomap Geom-GCN-I

Poincare  Geom-GCN-P 

struc2vec  Geom-GCN-S
### GIN/转述
核心公式 

$h_v^{(k)}=MLP^{(k)}((1+\epsilon ^{(k)})h_v^{(k-1)}+\Sigma_{u\in N(v)}h^{(k-1)})$

在实际观察中，发现$\epsilon$固定为0的时候略微但始终优于可学习的$\epsilon$，GIN模型在相比于GCN，和GraphSAGE取得了sota。

证明了MLP在计算hi的时候会有帮助。

### Understanding Attention and Generalizationin Graph Neural Networks 原文

做了有关threshold的研究。让attention取值在摸一个额定取值之外的边被抛弃。或者每一层pooling之后都留下r=0.8的边进入下一次迭代。

发现attention的取值都很相近，导致大量的边被同时的舍弃和保留。

之后文章提出了自己的模型ChebyGin，模型按照度数的比例聚合了距离自身k内的节点，作为自身的特征。是一个GCN的扩展。

相比于GIN和GCN在colors（程序生成计算绿色节点数量的数据集），triangle（数三角形），以及MINST-75AP（image分类）数据集sota。


**idea：不采用threshold，而是按照排名去加边或者减少边，考虑到所有的节点而不是adj的节点。**
# 做过的尝试
把图形结构的嵌入结果 [N,node_number]维度的向量称为 ri

节点的本身特征称为hi

## 图形信息对模型效果的重要性
朴素GAT

Test set results: loss= 0.7370 accuracy= 0.8330
### 只用ri进行GAT训练
Test set results: loss= 0.9220 accuracy= 0.7730

### 把ri和hi拼接，输入GAT
$$GAT (hi || ri || degree)$$
Test set results: loss= 0.8344 accuracy= 0.7850
Loading 524th epoch r=0.8
Test set results: loss= 0.7832 accuracy= 0.8030
Loading 1025th epoch r=0.95
Test set results: loss= 0.6741 accuracy= 0.8370
### hi 和 ri 分开 让模型变得更简单

$$ei= LeakyReLu(Att1_{ij}(Wh_i)) ||  LeakyReLu(Att2_{ij}(Wr_i))$$

$$h_{i+1}=fc(ei)$$
Test set results: loss= 0.6311 accuracy= 0.8220
## 聚合远距离节点
nhop=1
Test set results: loss= 0.8828 accuracy= 0.8150
nhop=5
Test set results: loss= 1.2182 accuracy= 0.6780

**idea：分析为什么用feaeture的attention会让结果更差，猜测是因为feature在使用的时候其实只是找到和自身最相似的node进行聚合，和简单把自身倍增没有本质区别。应从节点的结构考虑进行加边**

## 目前我的任务是找到图形信息对模型效果的重要性

分别把向bert一样放进feature里面

hi计算到下一个h_i+1的时候和他再来一层

# 遇到的问题 
找到新的数据集