1.可以考虑的思路 
验证GATv2，弄清楚GATv2

# 论文阅读记录
### GEom-GCN/原文
文章为了解决GCN存在的两个问题，提出来

1. 无法聚合距离较远的点
2. 没有考虑图形的几何信息

于是，论文提出按照graph embeding后的向量划分节点和节点之间的关系。具体来说，针对每一个节点$i$和本轮特征$h^i$，以及graph embeding后的向量ri，划分成四种关系（可以按照ri之间的距离）。每一种关系的节点聚合后拼接，通过非线性的单层神经网络生成下一次迭代的向量$h^{i+1})$

关于graph embeding，他提出了三种方式的尝试，并对应三种模型

Isomap Geom-GCN-I

Poincare  Geom-GCN-P 

struc2vec  Geom-GCN-S
### GIN/转述
核心公式 

$h_v^{(k)}=MLP^{(k)}((1+\epsilon ^{(k)})h_v^{(k-1)}+\Sigma_{u\in N(v)}h^{(k-1)})$

在实际观察中，发现$\epsilon$固定为0的时候略微但始终优于可学习的$\epsilon$，GIN模型在相比于GCN，和GraphSAGE取得了sota。

证明了MLP在计算hi的时候会有帮助。

### Understanding Attention and Generalizationin Graph Neural Networks 原文

做了有关threshold的研究。让attention取值在摸一个额定取值之外的边被抛弃。或者每一层pooling之后都留下r=0.8的边进入下一次迭代。

发现attention的取值都很相近，导致大量的边被同时的舍弃和保留。

之后文章提出了自己的模型ChebyGin，模型按照度数的比例聚合了距离自身k内的节点，作为自身的特征。是一个GCN的扩展。

相比于GIN和GCN在colors（程序生成计算绿色节点数量的数据集），triangle（数三角形），以及MINST-75AP（image分类）数据集sota。


**idea：不采用threshold，而是按照排名去加边或者减少边，考虑到所有的节点而不是adj的节点。**

找到新的数据集

coding