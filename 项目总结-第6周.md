

# 失败的尝试
## 参数调整|更换attention实现方式|添加Mlp层

调整参数 激活层，hidden_size,dropout **失败** 最佳结果<74

feature直接堆叠 **失败**  最佳结果<74

更换attention实现方式 原始GAT实现，用deepwalk的实现  **失败**  最佳结果<74

用deepwalk的向量进行拼接  **失败**  最佳结果<74

添加全连接  **失败**  最佳结果<74

总结经验： attention的操作是一种聚合操作，需要和周围的邻居按照比例进行加权。在进行**加**的操作之前，不能添加norm或act_layer，防止**不同表意**的特征混合。
# 对FSGNN分析
FSGNN的模型概要

1. feature预处理：聚合操作 each_layer=features.dot(khop_adj)
2. 降维 ：nn.linear(feature,nhid=64) for each layer 
3. 归一化: f.normalize(nhid) for each layer
4. 对layer选择（没什么用）：multiply a learnable weight for each layer
5. relu (concat each layer )
6. 分类器 ：fc

## 进一步分析

1. FSGNN过拟合非常严重，模型效果并不稳定，但运行速度快，在不同切割的数据集下模型的效果有较大的偏差 74--78
2. norm的作用：验证提升缺失非常大，和论文相符。原因，norm的使用主要是由于**features.dot(khop_adj)**操作，使得不同layer的特征尺度有较大的差距。
3. 过拟合，实际上，我打印了每一层的weight，得到[0.6,0.3,0.3,0.08,0.08....]，前三个分别是features features.dot(adj) features.dot(adj_i)。
   证明在训练过程中，原始feature发挥很大的作用，虽然出现了过拟合的情况，但是因为有过聚合的曹组，和GAT本身效果实在不行。所以FSGNN有一个很好地结果。

# 聚合不同邻居
## 事实上，我觉得我们的模型更类似于上述步骤的第一个步骤，所以我把聚合换成了attention操作，剩下的保留了FSGNN的模式，同时聚合了不同的邻居。

具体来说，$concat(attention(v,neigborAttention(N_i(v)))\text{, for i in range(k)})$

其中$N_i$表示=nhop的邻居

1. 考虑nhop邻居的时候是要排除(n-1)hop的邻居，也就是新产生的邻居。也尝试了不排除(n-1)hop的邻居，效果不好
2. 由于邻居数量多，采用随机采样的方式，分别采样了150和100个邻居，不足就padding。得到效果差距不大。

-dropout 0.57 --weight_decay 1e-3        **76.6886, 1.35**

## 值得注意的是，同时我也做了两个失败的尝试
1. 把features直接和attention聚合后的结果并在一起，过拟合十分严重，失败
2. 用两层Mlp作为下游人物，失败

## 引入mutiheadattention和多nhop
--dropout 0.5  --nhead 8 4 1 -   76.4254, 2.15

--dropout 0.57  --nhead 8 4 1   75.4386, 1.99

--dropout 0.57  --nhead 8 4 1 1   75.9649, 1.54

**没有显著提升**

## 剩余的优化空间
1. 考虑新的方式
2. 调整参数 nhidden
3. 聚合更远的邻居
4. 优化模型结构，因为train的val可以达到90，可以考虑用一些模型构建的技巧平滑过拟合。

# 关于预训练模型
调研阶段，我找到一个同学，一起合作阅读相关文献。


### 阅读图自监督学习，确实有类似的做法
1. mask图的一部分，重新生成原图（生成式自监督学习）
2. 对图进行多种变化，使得同一个图在经过变化后的新图尽可能的接近。接近是指生成的低维向量的距离（有专门定义）更低。

### 设计的雏形可以仿照beit进行研究