
# 可以改进的地方
1. 更改实现方法
2. 优化加边的方案

1. prime att impl new W a 
2. q k , wq wk W a
3. GCN 
4. 验证 FSGNN 去hop  
5. feature pre process
6. norm 
7. Brit 
8. lr normforA dropedge 附录 hidden mlp_layer

# 参数调整
-lr 0.01 --nb_heads 8  73.7500, 1.9

--lr 0.005 --nb_heads 4 71.8860, 2.17

--dropout 0.5  72.5658, 2.89

 --lr 0.01 --nb_heads 8 --hidden 16  71.4912, 2.56
 -lr 0.01 --nb_heads 8 --hidden 4   72.3465, 1.94
 -lr 0.03 --nb_heads 8 --hidden 8   71.0965, 1.86
 68.9254, 1.55 layer3 直接拼接

mode 3+leakyrelu 72.9167, 1.33
mode 3+leakyrelu+scare   73.0702, 1.38
mode 3+leakyrelu+replace W by nn.Linear 72.6316, 2.47
mode 3+relu+scare   72.9386, 1.19
mode 3+mlp+71.8421, 2.31
从此往下 scar自动添加

dropout Double Mlp  73.0921, 1.62

Mlp 72.0175, 1.62

doubleMlp drop
 ## 剩余的思路

 拼接deepwalk

 两层mlp，考虑加droup

 feature增强，看一下相关论文

 dropout

 包里调参