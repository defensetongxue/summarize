## mask_features with random
|                      | cora            | citeseer        | wisconsin       | texas           | cornell         | squirrel        | chameleon       |
| -------------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| 2-layer GAT (原模型) | 28.35, 8.49     | 21.61, 9.67     | 23.53, 5.68     | 28.65, 8.48     | 27.07, 10.26    | 21.29, 1.71     | 22.35, 4.15     |
| GAT+fc分类器         | **75.65, 2.04** | 58.06, 3.06     | 42.35, 6.63     | 55.14, 5.69     | 43.24, 6.94     | **48.21, 1.91** | **68.82, 3.25** |
| GAT+fc+norm          | 71.69, 2.84     | 59.57, 2.69     | **42.75, 7.48** | **59.46, 6.73** | **52.97, 6.19** | 35.13, 2.01     | 53.09, 6.12     |
| 1hopGAT+fc+norm      | 74.95, 6.23     | **59.71, 4.75** | 33.73, 7.06     | 54.59, 5.51     | 36.76, 5.3      | 41.20, 8.9      | 67.32, 9.09     |
| 2-layerGCN  (原模型) | **79.82, 1.58** | **64.61, 3.94** | 30.20, 7.08     | 26.76, 5.85     | 34.86, 3.3      | 38.86, 3.37     | 62.15, 2.93     |
| GCN+fc               | 68.07, 2.24     | 57.57, 4.23     | **31.57, 8.88** | 30.27, 8.53     | 30.54, 5.28     | 54.70, 2.07     | 68.62, 3.75     |
| GCN+fc+norm          | 79.50, 1.19     | 64.01, 3.91     | 29.22, 7.31     | 29.19, 6.26     | 34.32, 5.41     | **60.08, 1.9**  | **70.61, 2.41** |
| QKV+fc               | 27.77, 3.74     | 24.79, 2.13     | 25.69, 11.11    | 22.70, 8.04     | 27.03, 6.84     | 25.11, 2.45     | 32.76, 2.3      |
| QKV+fc +norm         | 37.32, 2.19     | 33.21, 2.31     | 28.82, 6.45     | 31.35, 10.13    | 32.97, 11.58    | 24.99, 1.77     | 31.29, 2.73     |
| 1hopQKV+fc           | 42.88, 4.08     | 25.36, 2.69     | 25.10, 10.7     | 25.14, 9.29     | 22.70, 7.07     | 24.62, 3.05     | 32.94, 5.87     |
| 1hopQKV+fc+norm      | **61.25, 1.99** | **50.18, 3.81** | **35.69, 6.89** | **37.30, 8.87** | **33.78, 5.83** | **68.50, 3.14** | **75.55, 1.57** |

观察和分析： 
1. 在GAT以及各种变式中，单层GAT+fc配合加norm与否能够在平均意义上利用图形信息，在GCN中，分析结构近似于k-hopfeatures异性的走向。QKV中，1hopQKV+norm优于其他组合
2. 横向分析数据集，异行较高的数据集wisconsin、texas、cornell的图形结构信息最不容易被利用，而1hop和2hop异性的结构图形结构信息最能够利用
3. GCN，和GAT在利用2hop异性数据集的时候强于QKV，QKV在利用1hop更好一些   


## change the adj_i
adj_i=(a*adj^i+b*I)
## adaptive layer
```python
model:
    x=Mlp(x) # Mlp= fc(nfeat,nhid) + relu
    x=classifier(x) # classifier = fc(nhid,nclass)
    x=propogate(x,adj)
    return logsoftmax(x)


propogate(x):
    out_list=[]
    for i in range(k):
        H^i=adj^i·x
        s=fc(H^i) # fc=(nclass ,1)
        s=sigmoid(s)
        out_list.append(H^i*s)
    return sum(out_list)
```

分析研究：

1. 理想情况下，s的计算是综合了features和不同阶段的图形结构信息的，然而，用不同hop的低维features想要学出这一个hop的权重还是有些困难。同时，此模型没有考虑position_embeding,也就是不同的hopfeatures到底是第几个hop，这个是由于论文写作是用的数据集是老三样数据集，这三个数据集同性较高。得出的结论是：`we hold that the over-smoothing issue only happens when node representations propagate repeatedly for a large number of iterations, especially for a graph with sparsely connected edges`。在这样的情况下，position是不重要的。
2. 另外，这个是与label无关的，论文中定义的smothness是向量和其他向量的夹角，没有考虑对于label的分辨能力。
3. 在计算每一个hop节点的权值的时候，没有考虑其他hop的权值情况。事实上，计算hop权值类似于Nlp中的event extract，Nlp目前的神经网络做法是把每个词和句子级别特征并起来，或许不用hop之间也可以尝试计算句子级别的特征

|                         | cora            | citeseer        | wisconsin       | texas           | cornell         | squirrel        | chameleon       |
| ----------------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| deeperGNN               | 85.63, 1.15     | 72.26, 1.38     | 47.84, 6.57     | 43.78, 8.36     | 50.00, 7.57     | 38.67, 1.7      | 57.13, 2.29     |
| deeperGNN +norm         | 86.96, 1.2      | 74.02, 1.71     | 56.86, 4.88     | 56.49, 7.3      | 58.65, 6.05     | 38.52, 1.97     | 58.25, 2.6      |
| deeperGNN+position      | 86.22, 1.26     | 73.19, 1.33     | 56.67, 5.91     | 51.35, 7.05     | 56.76, 7.45     | **39.43, 1.63** | **58.42, 2.31** |
| deeperGNN+position+norm | **87.00, 1.16** | **74.76, 1.28** | **61.76, 3.19** | **61.62, 8.53** | **63.51, 5.83** | 36.67, 1.78     | 58.00, 2.48     |

在进行norm的时候，把propagate步骤挪到classifier之前，避免在分类器后使用norm导致区分度不足。
### deeperGNN+position+norm
| adj^k     | 0(I)       | 1          | 2          | 3          | 4      | 5      | 6          | acc       |
| --------- | ---------- | ---------- | ---------- | ---------- | ------ | ------ | ---------- | --------- |
| cora      | **0.3377** | 0.2299     | 0.2197     | **0.2854** | 0.2466 | 0.2298 | 0.2030     | **85.92** |
| citeseer  | **0.5633** | 0.2080     | **0.2644** | 0.1800     | 0.1790 | 0.1867 | 0.1904     | **74.92** |
| wisconsin | **0.5923** | 0.3377     | **0.3657** | 0.3422     | 0.3482 | 0.3539 | 0.3575     | **54.90** |
| texas     | **0.5335** | 0.1667     | 0.1719     | 0.1623     | 0.1733 | 0.1952 | **0.2198** | **67.57** |
| cornell   | **0.5579** | 0.3401     | 0.3728     | 0.3500     | 0.3483 | 0.3639 | **0.4082** | **56.76** |
| squirrel  | **0.4382** | **0.4553** | 0.1011     | 0.0788     | 0.0757 | 0.0785 | 0.0803     | **36.22** |
| chameleon | **0.4757** | **0.3498** | 0.2335     | 0.1462     | 0.1301 | 0.1227 | 0.1129     | **59.87** |

去掉position_embeding区分度会更低
## norm 到底作用是什么

猜想：mixhop的公式为 $H^{i+1}=||_{j=0...k}A^{j}H^{i}W^j$，norm用来减少不同layer采用不同W作为降维的影响，因为数据量并不大，所以W很难学的很好。

FSGNN: GCN+mixhop+norm+soft_selected

|                            | cora            | citeseer        | wisconsin       | texas           | cornell         | squirrel        | chameleon       |
| -------------------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| FSGNN                      | **84.37, 1.36** | 73.59, 1.53     | 56.08, 3.74     | 64.59, 7.69     | 48.38, 11.75    | **62.76, 1.97** | **75.44, 1.75** |
| FSGNN 无norm               | 80.28, 1.81     | **74.96, 1.55** | **81.18, 4.31** | **80.54, 5.77** | **76.76, 4.71** | 46.30, 2.27     | 62.43, 2.26     |
| FSGNN(using same W)        | 82.47, 1.18     | 68.29, 1.43     | 43.33, 3.33     | 40.27, 12.35    | 40.27, 10.43    | 63.90, 2.2      | 69.61, 2.34     |
| FSGNN(using same W) 无norm | 82.03, 2.14     | 73.37, 1.59     | 77.84, 3.93     | 71.62, 6.42     | 68.92, 7.07     | 37.34, 2.4      | 60.48, 1.45     |

猜想错误，但根据观察，norm不能直接提升`无条件提升`模型效果，目前仅能提升1hop邻居的效果



## 基于节点特征的自适应消息传递

在计算attention的时候，自适应的根据不同layer生成adj

1. 数据集特征
2. 节点特征
3. 节点图结构特征

## feat_type
|                             | cora                | citeseer        | pubmed         | cham                 | wisconsin       | texas           | cornell            | squirrel            | film                |
| --------------------------- | ------------------- | --------------- | -------------- | -------------------- | --------------- | --------------- | ------------------ | ------------------- | ------------------- |
| 1hopQKV   3hop feat_type*3w | 87.42, 1.36(lo)     | 75.85, 1.59(lo) | 89.6, 0.53(lo) | 79.34, 1.14(no)      | 87.06, 3.42(no) | 84.86, 5.01(no) | 84.86, 4.71(lo)    | 73.09, 1.81(no)     | 35.45, 0.83(no)     |
| 1hopQKV   8hop feat_type*3w | 87.73, 1.2(lo)      | 76.64, 1.66(lo) | miss           | 79.28, 1.08 (no)     | 85.49, 4.49(no) | 85.41, 4.22(no) | **87.3, 6.29(no)** | **74.43, 1.98(no)** | **35.71, 0.88(lo)** |
| 1hopQKV   3hop feat_type*w  | 87.18, 0.87(no)     | 76.1, 1.89(no)  | 89.59, 0.4 (no | **79.34, 1.14 (no)** | 86.67, 3.9(lo)  | 84.86, 4.39(lo) | 85.14, 5.57        | 73.09, 1.81         | 35.34, 0.99         |
| 1hopQKV   8hop feat_type*w  | **88.13, 1.28(no)** | 76.49, 1.41(no) | miss           | 79.28, 1.08(no)      | 86.67, 4.0(no)  | 84.86, 5.01(no) | 84.86, 5.57 (lo)   | 74.43, 1.98 (no)    | 35.36, 1.07(lo)     |
| FSGNN   3hop feat_type      | 87.14, 1.23         | 76.56, 1.17     | **89.65, 0.5** | 78.4, 1.05           | **87.65, 4.12** | 84.59, 4.69     | 85.68, 4.37        | 72.54, 1.92         | 35.04, 0.58         |
| FSGNN   8hop feat_type      | 87.97, 1.3          | **76.88, 1.37** | miss           | 78.82, 0.9           | 86.47, 3.09     | **85.68, 4.2**  | 86.22, 6.1         | 72.13, 2.14         | 35.32, 1.18         |

## not featype
|         | cora           | citeseer    | pubmed      | cham           | wisconsin      | texas          | cornell     | squirrel       | film        |
| ------- | -------------- | ----------- | ----------- | -------------- | -------------- | -------------- | ----------- | -------------- | ----------- |
| 1hopQKV | 87.51, 1.08    | 76.88, 1.63 | 89.59, 0.5  | 79.12, 1.15(8) | 87.25, 2.67(8) | 84.86, 4.22(8) | 85.95, 4.32 | 74.77, 2.08(8) | 35.49, 0.78 |
| FSGNN   | 87.55, 1.39(8) | 77.15, 1.34 | 89.47, 0.46 | 77.61, 1.13    | 86.27, 3.62    | 84.59, 4.53(8) | 86.22, 5.85 | 72.23, 1.44(8) | 35.45, 0.79 |

# best performance
|         | cora        | citeseer    | pubmed     | cham        | wisconsin   | texas       | cornell     | squirrel    | film        |
| ------- | ----------- | ----------- | ---------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| 1hopQKV | 88.13, 1.28 | 76.88, 1.63 | 89.6, 0.53 | 79.34, 1.14 | 87.06, 3.42 | 85.41, 4.22 | 87.3, 6.29  | 74.77, 2.08 | 35.49, 0.78 |
| FSGNN   | 87.69, 1.11  | 77.05, 1.69| 89.68, 0.39 | 78.18, 0.94  | 86.47, 3.66 | 85.68, 4.84  |87.3, 6.4 | 74.13, 1.49|   37.63, 1.12 |
