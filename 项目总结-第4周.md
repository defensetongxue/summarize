<style>
    * { 
        font-family: "等距更纱黑体 SC"
    }
    </style>
1. 针对deepwalk详细了解
2. 针对GCN的数据集分类方法进行详细了解
3. 实现新的attention降维
4. 关于按照图形结构的graph_feature进行加边
# 论文阅读
## Beyond homophily in graph neural networks
However, most of these models by design are
more suitable for homophily datasets where nodes linked to each
other are more likely to belong in the same class. They may not
perform well with heterophily datasets which are more likely to
have nodes with different labels connected together

虽然触及了homophily，提出了改进聚合的方式解决。

$$r^{(k−1)}_v,AGGR({r^{(k−1)}_u:u\in N_1(v)}),AGGR({r^{(k−1)}u:u\in N_2(v)})$$

## FSGN:Improving Graph Neural Networks with Simple Architecture Design

However, most of these models by design are
more suitable for homophily datasets where nodes linked to each
other are more likely to belong in the same class. They may not
perform well with heterophily datasets which are more likely to
have nodes with different labels connected together

通过邻接矩阵的改进

目前在chameleon数据取得最好成绩的模型,他的核心思想是通过拆分邻接矩阵，使得模型能够自适应的学习重要的边。

算法分为三个核心的部分 

1. 对于node_features在每个hop的聚合都单独的拿出来，平行的作为模型的输入。对于一个n维的node_features。分别对有self-loop的聚合方式和无self-loop的聚合方式，在K-hop的聚合距离下，生成(2*K+1)*n维的输入向量
2. 对于1中的每一组node_feature(2*K+1 totally)，都乘一个可学习的scale，scale反映node_features的重要性。
3. 对于从node_features转化为hidden_feature后，增加一个norm层。发现在chameleon和squirrel分别取得(53+19=72%，32+31=63%)的效果提升。

## Two Sides of the Same Coin ：Heterophily and Oversmoothing in Graph Convolutional Neural Networks
文章的主要观点如下：
1. 定义**Relative Degree**和**Heterophily**
2. when the error rate and homophily level are high, oversmoothing is
initially triggered by low-degree nodes. Thus, we aim to compensate for low degrees via degree corrections。using learnable $A^{l}=A^{l}\bigodot\mathcal{T}$ 其中$\bigodot$ 为MLP
3. based on the similarity of i and it's norbors as aggregate the features
<!-- 
从实现上是
```python
    '''con is layernorm(nn.Batchnorm1d) | fc  which output dim =hidden feature '''
    layer_inner = con(layer_previous,adj,self.degree_precompute)
```
. -->





# double-attention-implement
 $\text{for node }i,\text{n nodes totally}$
$$h^{k}_{1*n}=Att(where \ adj[i]>0)_{1*n}*Att(where \ neighbor\_adj[i]>0)_{n*n}*H^{k-1}_{n*n}$$

$adj[i]\text{ is node i's neighbors and self-loop which is 1*n dim}$

$neighbor\_adj[i] \text{ is net-Matrix generated by }adj[i]\text{(no self loop)}$

$neighbor\_adj[i]=adj_{no-self-loop}[i]+adj_{no-self-loop}[i].T+Eye_{ii=1,else=0}$

进程在运行中会被杀死。

有可能可以通过换用稀疏矩阵解决，可能存在的问题是由于fatures是密集的矩阵，如果用稀疏矩阵的表示方法会导致一定的计算开销。

适用cuda，但是连外网会比较困难，目前在找一些国内的源。
    

# deepwalk graph embeding

deepwalk to 64dims -> ri , nodefeature is hi
$$H^{i+1}mlp(fc(ri)||Att*H^i)$$ 
成功换用标准划分

chameleon
Train cost: 10714.0629s
Test accuracy: 62.982456140350884, 2.89

0  1 2 3 4 5 6 
1  5 6 7 8 9 10 

F= 2*6*hidden

hidden*hidden

Q=2*6*hidden
K=2*hidden*6


1 2 3 4 5 6 7
5 6 7 -1 - 1- 1 -1

bert
