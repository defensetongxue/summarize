## 要解决的问题
观察到，在图神经网络中，features过于稀疏，使得不能从features中学习到和邻居的差异化的信息。并且在梯度传递的时候，有边的邻居具有相同的梯度，使得attention的取值在一个邻域内取向同质化。


## 如何评判一个邻域内的attention取值同质化

对每一个节点的计算的attention取值进行排列，如果能够把和自己类别相同的节点排在前面，那么就证明attention是有意义的。

1. 一个粗略的估计是：对每个节点attention取值在前十名的邻居（不包括自己），看一看这10个人中有多少是和自身同类的。
2. 用同类邻居的 平均attention取值之和除以异类邻居的attention取值之和

## 用编码代替features || 初始化 || 预训练

效果： 这三者并没有在attention的有效性有可观测的提高，并且效果上也没有明显的提升

总结：利用图空间对features空间进行降维的操作都将失败，因为GCN和GAT本身就是根据图空间进行消息传递。目前已有的自监督学习的方法是使得向量能够和自己的邻居在低维表达的情况下更加接近。这并不能够帮助GAT模型区分不同邻居对中心节点的效果。所以，必须要找到除了图空间以外的邻居帮助自己进行降为，才可能能在图空间取得一定的区分度。

CV和NLp领域之所以能够在attention的使用方面取得大的进步，是因为，同样的词或者相似的图形的分布**在其他地方见过足够多**。而在图学习领域，features一个节点仅会在其自身和邻居的判断过程中出现。因为节点特征的稀疏，导致节点之间特征的相似性会比较低，因为特征稀疏导致特征的分布会比较零散，本身含有的信息量不高。要解决attention同质化的问题，不能太过依靠图空间的信息，比如GAE直接用图空间作为目标，这会导致有边的节点的向量低纬表征不同。

GEOM做了类似的尝试。

GEOM（提出了三种实现，有一种效果是好的）是根据features空间进行等距降维，也就是把features更为接近的向量在低维空间安排一个更近的距离。然后再低维空间去寻找向量离得更近的节点作为一组扩展邻居。大体相当于找**一组和自身features更接近的邻居**。所以，GEOM也在features空间中寻找解决，但是因为之前提到的features稀疏的原因。**在原始features上找邻居并不是一个好主意。**



同时，这个实验给我的启示是，在计算节点之间的attention的时候，不使用原始的features计算attention，而是使用$adj·features$来计算attention。在聚合特征的时候，仍然用原始features来做。

这样的好处是，用一个简单的方式解决了features稀疏性的问题，使得计算的attention更有区分度一些。对于mixhop来说，计算2hop邻居的attention的时候用$adj^2·features$


## 预训练解法：
        
                train      val      test
    train   | pretrain  |       |           |
    val     |           | preval|           |
    test    |           |       |           |


1. 采用训练集的label构造邻接矩阵，label相同的为1，label不同的为0。
2. 对上面构造的邻接矩阵，和adj·features，进行类似GAE的操作。
3. 用val数据集进行验证，找到loss最大的


在预训练后，前十名能够有6个是和自身同样label的，在正式训练后这个取值会提高到7

但是从acc来评判，这种预训练效果并不好，因为经过预训练后，节点对自身的异类邻居的attention值会很低，在跑训练集，由于每个节点都天生的给自己的同类邻居较高的attention，会使得分类器不能充分的训练。虽然我尝试了一些数据增强的手段。比如mask部分features，mask部分graph(drop-edge 在文献引用的图中有好效果，但在分子图中有反效果，不是一个通用的解法)



另外有一个非常重要的发现是。norm是至关重要的。如果不同norm，attention前十名排名里面，label的占据和随机计算的attention是相同的。这一步，我也进一步探究，发现如果不用norm，降维的向量的模的大小会相当程度决定分类的结果。这个在计算attention的时候会产生很坏的影响。
算法描述是：

## 另一种解法
1. features adj
2. attention=calculate_attention(adj·features)
3. H=attention·features·W
4. 分类器


calculate_attention(x):
1. x=fc(x)
2. Q,K=fc_Q(x),fc_K(x)
3. attention=Q*K
   

另外可能需要在mixhop中加入GEOM生成的邻居

这样的好处有：
1. features本身并不具有清晰地表达自己的能力（因为稀疏），所以用features·adj。
2. 这样能关注到公有邻居，两个节点有公有邻居的时候，attention取值会更高。

| dataset | 1hopQKV  + norm + mixhop      | GCN + norm + mixhop          | GAT  +norm +mixhop         | 1hopGAT+ norm + mixhop        |
| ------- | ------------- | ------------- | ------------- | ------------- |
| cora    | **85.2113, 1.27** | 84.1650, 1.38 | 84.5875, 0.91 | 84.6479, 1.11 |
|citeseer|73.7152, 1.15 | **74.4787, 1.76** | 73.1683, 2.04 | 73.4086, 1.41|
|wisconsin| **74.1176, 7.63** | 64.7059, 9.11 | 64.5098, 7.96| 67.2549, 7.29|
|texas|71.3514, 5.16 | **72.7027, 7.09**  | 68.3784, 7.16 |  70.2703, 7.25|
|cornell|67.5676, 7.45 | 63.7838, 6.42 | 63.2432, 7.76 | **69.1892, 6.42**|
|squirrel|**73.5927, 2.26** | 67.3103, 1.84 | 55.8309, 2.01 | 71.6619, 1.84
|chameleon|**78.7061, 1.48** | 76.0965, 1.01 | 68.5526, 1.82|78.0482, 1.2|

## trick
提前终止： 不用loss评判，而是用（1-acc）*loss

允许对于label的一些模棱两可的判断能够帮助我们的模型



1 2 

1 : 2 3

2: 1 3

1. features 取消 用随机值代替features
2. 变化跳数
3. 拼features 预训练的 graphmae
4. 刷榜
5. 加trick