1. 已经得到GAT的attention acore matric，并转化成邻接表的形式
2. 找到未分类成功的集合
3. 分析GAT的矩阵
   ```python
   example: 0,Wrong Point is 506,label is 4
   (506,506, att_score=0.5, 506 labels is 4)
   (506,514, att_score=0.5, 514 labels is 6)


   example: 1,Wrong Point is 509,label is 5
   (509,314, att_score=0.32, 314 labels is 5)
   (509,509, att_score=0.33, 509 labels is 5)
   (509,669, att_score=0.35, 669 labels is 5)


   example: 2,Wrong Point is 510,label is 3
   (510,192, att_score=0.2, 192 labels is 2)
   (510,356, att_score=0.22, 356 labels is 2)
   (510,510, att_score=0.2, 510 labels is 3)
   (510,519, att_score=0.19, 519 labels is 2)
   (510,2223, att_score=0.19, 2223 labels is 2)


   example: 3,Wrong Point is 511,label is 3
   (511,74, att_score=0.35, 74 labels is 3)
   (511,511, att_score=0.36, 511 labels is 3)
   (511,611, att_score=0.29, 611 labels is 2)


   example: 4,Wrong Point is 518,label is 6
   (518,76, att_score=0.18, 76 labels is 3)
   (518,181, att_score=0.16, 181 labels is 6)
   (518,518, att_score=0.17, 518 labels is 6)
   (518,662, att_score=0.16, 662 labels is 2)
   (518,766, att_score=0.15, 766 labels is 2)
   (518,2532, att_score=0.17, 2532 labels is 3)
   ```
   发现attention score接近于平均，对原因进行探索：
   1. 通过观察feature降为后的h，或者打印降维度之前的feature，那就更看不出来了
   ```python
   print(out_features[518]) # (518,518, att_score=0.1666, 518 labels is 6) [-1.4367, -0.7395,  2.0282,  2.1675, -0.7336, -1.1632,  0.5716],

   print(out_features[181]) # (518,181, att_score=0.1617, 181 labels is 6) [-1.3443, -0.6514,  1.8771,  1.6938, -0.7852, -0.8849,  0.7766],

   print(out_features[76]) # (518,76, att_score=0.1795, 76 labels is 3)   [-1.1565, -1.1395,  1.5078,  2.7587, -0.7998, -1.0949,  0.5674],
   ```
   
   
   另外，我对正确的点位进行了相同的分析。发现，正确的点位相比于错误的点位，其邻居的自己以外的类占比很少。但是，也有例外的现象。
   ```python
   example: 11,right Point is 1203,label is 2
   (1203,542, att_score=0.1106, 542 labels is 2)
   (1203,779, att_score=0.1173, 779 labels is 2)
   (1203,816, att_score=0.1059, 816 labels is 2)
   (1203,881, att_score=0.1083, 881 labels is 2)
   (1203,1119, att_score=0.1085, 1119 labels is 2)
   (1203,1195, att_score=0.1153, 1195 labels is 2)
   (1203,1203, att_score=0.1083, 1203 labels is 2)
   (1203,1411, att_score=0.1075, 1411 labels is 2)
   (1203,1630, att_score=0.1183, 1630 labels is 2)

   right Point is 782,label is 2
   (782,782, att_score=0.4763, 782 labels is 2)
   (782,1532, att_score=0.5237, 1532 labels is 3)
   ```
   我对此的解释是，首先由于在embeding的过程中，仅仅把单词embeding出现的频率作为词向量损失了大量的特征，这导致不同类别的文章本身有一定难区分性。
   
   同时只用肉眼是很难发现的其h的内在特征的，在分析的时候也不应该局限于观察应该采用更加数学的方式去分析节点特征h对attention_score的影响。

   另外，节点的attention_score中正确和错误节点进行了简单的统计分析。发现，在正确的节点中，label相同的邻居（包括自己），attention_score之和的占比显然对高于错误节点。分别为0.9097和0.6404.这个结论是显然的。如果要继续改进，应该尽可能让同类的attention_score占比尽可能的高。
   平均一个点有多少标签不同的点 wrong set 0.9097487926483154, right set 0.6403754353523254
   结论不同的点占据总点的比例: wrong set 0.42795389048991356, right set 0.10374702187567685
   平均标签不同的点的比例: wrong set 0.3596670619847353, right set 0.08931558355874128

   我的猜想是，加大在训练的时候对不同label的邻居的排斥，但也不太清楚这个是否能work
   或者想办法调整attention计算方式，但是目前还没有其他的办法。目前的attention矩阵会经过softmax，或许可以换用更敏感的方式
   修改了attentinon的relu时候的参数。
   Test set results: loss= 0.6529 accuracy= 0.8450

   分析认为这个提高虽然是在论文的所标注的范围之外的。但是由于这是多次训练取得的最高值，其实并不准确。总体来说还是在误差范围内的。只有由于模型过拟合导致的模型的效用明显变低。

   目前有两个想法，第一个是计算qkv，第二个是考虑fingerpoint，但这一块可能需要自己实现。
4. 还需要对对ADSF进行类似的工作