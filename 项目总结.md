完成的任务：
1. 浏览graph领域的论文
2. 关于之前判断错误的节点边相对少，干扰点少的结论，还没有具体尝试。但通过阅读论文得到了一些相关的分析结论。
   
   `Understanding Attention and Generalization in Graph Neural Networks` 从数据总体的结构分析，得出结论。more robust to larger and noisy graphs。认为相比于简单节点，atttention机制可以在更复杂和大图上获取更多的结论。

   `Graph Attention Retrospective`从数据本身质量（和邻居均值之间的举例，也可认为干扰点的强度）。认为在数据 $||\mu || = \omega(\sigma \sqrt{\log{n}})$的情况下，intra-edge的attention score会区别于类外边。
<!-- 
   **CSBM**是Graph Attention Retrospective生成数据的工具。但是我没仔细看。 -->


1.attention score 类似一种无监督学习 

each node has a graph embeding -> **r_i**

## 横向对比 ：


    ADSF 用的是 "常量"，具体来说是RWR->calculate intersaction，ri在训练过程中不可变。并且用"ri"计算attention score,本质上解决的是GAT在聚合的时候没办法聚合非邻居节点的特征的问题。

    GEom-GCN 是计算RW->word2vec，用单层神经网络对graph embediing后的结果进行降维。然后直接作为features训练。解决的是feature信息量不足，忽略了图像信息的问题。

    GATv2更改了GAT的公式，解决的是，attention在计算的时候是针对于h向量计算的。所以不管hi对应的是哪一个hj。在计算的时候hi都被乘了一个相同的向量。而通过交换公式中的顺序。向把hi和hj拼接后降维，达到计算attention的时候焦距在hi和hj的综合情况。做出分析"理论上较弱的模型在实际数据上有更强的效果，因为太强的模型在实际应用中容易出现过拟合的情况"。




## 打算做的一些尝试：

1. 改一下上面的公式，延续之前自适应的attention的思路。通过hi计算attention score。e_ij=Att( concat(hi,hj) )。`可以考虑用GATv2，就是初始GAT交换顺序之后的公式`

然后对每一个 hi * W1 之后非线性化，得到ai。 对每一个ei向量乘上ai 再激活，得到计算得到的attention score。

注意到之前的其他一些有意义的尝试：

1. 通过在attention层之后添加一些全连接层，增强模型聚合信息的能力。
2. threshold，对于一些attentionscore超过某一个临界值的边不mask。但我不太确定这一块做到哪里了，因为之前那篇论文我给忘了。

        我关于这个方向的想法是：逐层自适应的学习threshhold，逐渐的加边。比如三层attention，每一层计算的都采用加边的操作
   
## 遇到的问题
1. rwr的公式存疑 extension://cdonnmffkdaoajfknoeeecmchibpmkmg/assets/pdf/web/viewer.html?file=http%3A%2F%2Fwww2.cs.uh.edu%2F~ceick%2F7363%2FPapers%2Ftong.pdf
   
   
## 剩下的简单的汇报 
1. 对ADSF的实现还在调试，主要遇到的问题在于，有一部需要计算距离矩阵，是一个n^3复杂度。原文代码中的数据和代码有一定缺失，应该不是最终版的代码。不过目前已经解决，还在远程服务器运行。
2. 跑chameleon数据集，chameleon数据集是一个weki百科的分类数据集。里面类别很多，数据更为复杂。边和点的比例依然和cora相似。还有松鼠类和鳄鱼类的两个并列的数据集。代码已经写好，等ADSF跑完跑这个。