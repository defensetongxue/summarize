1. 已经得到GAT的attention acore matric，并转化成邻接表的形式
2. 找到未分类成功的集合
3. 分析GAT的矩阵
   ```python
   example: 0,Wrong Point is 506,label is 4
   (506,506, att_score=0.5, 506 labels is 4)
   (506,514, att_score=0.5, 514 labels is 6)


   example: 1,Wrong Point is 509,label is 5
   (509,314, att_score=0.32, 314 labels is 5)
   (509,509, att_score=0.33, 509 labels is 5)
   (509,669, att_score=0.35, 669 labels is 5)


   example: 2,Wrong Point is 510,label is 3
   (510,192, att_score=0.2, 192 labels is 2)
   (510,356, att_score=0.22, 356 labels is 2)
   (510,510, att_score=0.2, 510 labels is 3)
   (510,519, att_score=0.19, 519 labels is 2)
   (510,2223, att_score=0.19, 2223 labels is 2)


   example: 3,Wrong Point is 511,label is 3
   (511,74, att_score=0.35, 74 labels is 3)
   (511,511, att_score=0.36, 511 labels is 3)
   (511,611, att_score=0.29, 611 labels is 2)


   example: 4,Wrong Point is 518,label is 6
   (518,76, att_score=0.18, 76 labels is 3)
   (518,181, att_score=0.16, 181 labels is 6)
   (518,518, att_score=0.17, 518 labels is 6)
   (518,662, att_score=0.16, 662 labels is 2)
   (518,766, att_score=0.15, 766 labels is 2)
   (518,2532, att_score=0.17, 2532 labels is 3)
   ```
   发现attention score接近于平均，对原因进行探索：
   1. 通过观察feature降为后的h，或者打印降维度之前的feature，那就更看不出来了
   ```python
   print(out_features[518]) # (518,518, att_score=0.1666, 518 labels is 6) [-1.4367, -0.7395,  2.0282,  2.1675, -0.7336, -1.1632,  0.5716],

   print(out_features[181]) # (518,181, att_score=0.1617, 181 labels is 6) [-1.3443, -0.6514,  1.8771,  1.6938, -0.7852, -0.8849,  0.7766],

   print(out_features[76]) # (518,76, att_score=0.1795, 76 labels is 3)   [-1.1565, -1.1395,  1.5078,  2.7587, -0.7998, -1.0949,  0.5674],
   ```
   
   
   另外，我对正确的点位进行了相同的分析。发现，正确的点位相比于错误的点位，其邻居的自己以外的类占比很少。但是，也有例外的现象。
   ```python
   example: 11,right Point is 1203,label is 2
   (1203,542, att_score=0.1106, 542 labels is 2)
   (1203,779, att_score=0.1173, 779 labels is 2)
   (1203,816, att_score=0.1059, 816 labels is 2)
   (1203,881, att_score=0.1083, 881 labels is 2)
   (1203,1119, att_score=0.1085, 1119 labels is 2)
   (1203,1195, att_score=0.1153, 1195 labels is 2)
   (1203,1203, att_score=0.1083, 1203 labels is 2)
   (1203,1411, att_score=0.1075, 1411 labels is 2)
   (1203,1630, att_score=0.1183, 1630 labels is 2)

   right Point is 782,label is 2
   (782,782, att_score=0.4763, 782 labels is 2)
   (782,1532, att_score=0.5237, 1532 labels is 3)
   ```
   我对此的解释是，首先由于在embeding的过程中，仅仅把单词embeding出现的频率作为词向量损失了大量的特征，这导致不同类别的文章本身有一定难区分性。
   
   同时只用肉眼是很难发现的其h的内在特征的，在分析的时候也不应该局限于观察应该采用更加数学的方式去分析节点特征h对attention_score的影响。

   另外，节点的attention_score中正确和错误节点进行了简单的统计分析。发现，在正确的节点中，label相同的邻居（包括自己），attention_score之和的占比显然对高于错误节点。分别为0.9097和0.6404.这个结论是显然的。如果要继续改进，应该尽可能让同类的attention_score占比尽可能的高。
   
4. 还需要对对ADSF进行类似的工作