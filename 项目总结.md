1. 已经得到GAT的attention acore matric，并转化成邻接表的形式
2. 找到未分类成功的集合
3. 分析GAT的矩阵
   ```python
   example: 0,Wrong Point is 506,label is 4
   (506,506, att_score=0.5, 506 labels is 4)
   (506,514, att_score=0.5, 514 labels is 6)


   example: 1,Wrong Point is 509,label is 5
   (509,314, att_score=0.32, 314 labels is 5)
   (509,509, att_score=0.33, 509 labels is 5)
   (509,669, att_score=0.35, 669 labels is 5)


   example: 2,Wrong Point is 510,label is 3
   (510,192, att_score=0.2, 192 labels is 2)
   (510,356, att_score=0.22, 356 labels is 2)
   (510,510, att_score=0.2, 510 labels is 3)
   (510,519, att_score=0.19, 519 labels is 2)
   (510,2223, att_score=0.19, 2223 labels is 2)


   example: 3,Wrong Point is 511,label is 3
   (511,74, att_score=0.35, 74 labels is 3)
   (511,511, att_score=0.36, 511 labels is 3)
   (511,611, att_score=0.29, 611 labels is 2)


   example: 4,Wrong Point is 518,label is 6
   (518,76, att_score=0.18, 76 labels is 3)
   (518,181, att_score=0.16, 181 labels is 6)
   (518,518, att_score=0.17, 518 labels is 6)
   (518,662, att_score=0.16, 662 labels is 2)
   (518,766, att_score=0.15, 766 labels is 2)
   (518,2532, att_score=0.17, 2532 labels is 3)
   ```
   发现attention score接近于平均，对原因进行探索：
   1. 通过观察feature降为后的h，或者打印降维度之前的feature，那就更看不出来了
   ```python
   print(out_features[518]) # (518,518, att_score=0.1666, 518 labels is 6) [-1.4367, -0.7395,  2.0282,  2.1675, -0.7336, -1.1632,  0.5716],

   print(out_features[181]) # (518,181, att_score=0.1617, 181 labels is 6) [-1.3443, -0.6514,  1.8771,  1.6938, -0.7852, -0.8849,  0.7766],

   print(out_features[76]) # (518,76, att_score=0.1795, 76 labels is 3)   [-1.1565, -1.1395,  1.5078,  2.7587, -0.7998, -1.0949,  0.5674],
   ```
   我对此的解释是，由于在embeding的过程中，仅仅把单词embeding出现的频率作为词向量损失了大量的特征。或者只用肉眼是很难发现的，应该采用更加数学的方式去分析节点对attention_score的影响。
   
4. 还需要对对ADSF进行类似的工作