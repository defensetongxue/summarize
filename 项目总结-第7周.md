#  失败的尝试
在尝试新1个hop的邻居的时候，同时采用上一hop被遗弃的邻居

原因：由于邻居的举例不统一，导致在聚合的时候有的点聚合的是k条邻居，有的是k-1条，并不统一。
Test accuracy: 77.1711, 1.5
# self_punishment 
在做attention的时候，对自己的attention的取值乘一个常数。

因为过拟合很严重，尽可能去减少自己的信息。

聚合三层邻居

分别self-loop-punish 1 0.8 0.3

77.3684, 1.16

分别self-loop-punish 1 0.5 0.3
77.1053, 1.38

分别self-loop-punish 0.5 1 0.3
77.3684, 1.14

和不用self-punishment在这个随机种子的切分下，准确率差不多，但用上去会略高一些。
# average 
分别对有self-loop 和没有self-loop的分别进行平均

简单来说就是有

1hop 1hop-self-loop

2hop 2hop-self-loop

3hop 3hop-self-loop
竖着做平均

同时我也尝试过横着或者所有，效果均是略次

由于涉及到向量之间的加法，所以norm函数很重要，我对norm用L1范数，L2范数，不用norm层，最终得到的结论分别是
78.6623 78.3114 58
同时，也尝试了neigborAtt+GCN和单层GAT效果，对多跳(self-loop,无self-loop)的邻居，均差一些
# 然后我继续这个思路，把pooling操作应用在了FSGNN上 成功

FSGNN 是 每一个$adj^i.dot(feature)$都线性投影到了64维度。

我把有self-loop的adj去掉，保留非self-loop的adj，然后进行n次矩阵乘机，展开为(n+1)维的操作。然后拼接成的 64*（n+1)维度的向量进行了maxpooling(4),maxpooling(8)，acc进一步提升到79.5。

然而，我把maxpooling应用到我们的模型上的时候，因为我们的模型的hidden是8维的，maxpooling，2，4，都没有能够让我们的acc到78以上。

同时，我让FSGNN借鉴的我们对adj nhop去重的操作，也没有效果上的提升。

但是我认为这个证明了，把$adj^i.dot(feature)$作为patchembeding，的可行性，在模型程度上，我们或许可以不太关注一个节点如何从邻居获取信息，而是做一个广泛的获取信息，然后展开成长向量在做进一步操作。

然而，我之后在FSGNN的(2n+1)*features进行attention的时候没有得到效果提升。但是我没有进行残差链接，这个目前存疑。


# 我开始探索BEIT在图领域实现的可能性

对于FSGNN，$adj^i.dot(feature)$通过self-loop和非self-loop的矩阵点积，使得features展开称为了（2*n+1)*features的长向量，每一个features就相当于一个patches，如果直接当做图领域的patches输入到bert里面有可能作为一个初期的尝试。

但是谷歌做bert实验的时候是8台GPU跑了好多天的程序，而且根据beit的实验统计，只有当数据量在百万级别的时候才会有作用。

而且用（2*n+1)*features作为patch_embeding 并不是一件很好地事情。

总结：目前遇到的两个困难
1. 主要是patch_embeding 的方法不行
2. 其次数据集很小并不合适。

为了解决1，我尝试，先跑一个GAT，在使用GAT的学好的attention取值作为adj进行矩阵多次乘法。失败。

