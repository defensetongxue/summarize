# 应用features预处理
在chemleon(78->73)和squirrel(72->68)数据集失败,在cora上有提升(83->85)

尝试的范围:
1. Mixhop: $H^{i+1}=||_{k=1...p} A^kH^iW_k$
2. Mixhop2: $H^{i+1}=||_{k=1...p}A^kMlp(H^i)$区别在于，这里的Mlp对每一阶的临界矩阵都相同
3. 自创Mixhop $H^{i+1}=||_{k=1...p}A^kDGAT_k(H^i)$(cora上有提升)
4. 自创Mixhop2 $H^{i+1}=||_{k=1...p}A^kDGAT(H^i)$
   
随着邻接矩阵不断叠加，考虑的邻域范围扩大，导致相邻边的特征越来越接近。对于异性比较大的数据集来说，会产生一定的干扰。具体来说，**只有features本身和dot(adj,features)是重要的**
   
## 值得关注的问题是
图神经网络似乎对于网络的层数十分的敏感，即使是加一层mlp也会（5%以上的程度）影响最后的准确率。

# “两种”GCN 破案
在chameleon数据集中，GCN的时候使用adj(68)，adj_i(72)差距大。

## 延伸
在第二遍attention的时候，可以考虑完全不采用节点自身的取值

## 总结
double attention的方案似乎并没有成功通过节点之间的attention使得attention变得有效。但从模型结构上来，double attention引入了更多的参数，可能导致了模型的过拟合。同时，我依然发现模型上存在很多的瑕疵，我认为还是有能够在squirrel上取得sota的可能的。

# 预训练


## bert LM
**bert为什么要mask？**

这是由于我们要**学习attention的权重**，通过mask一些patch，用其他的patch，通过attention去学习这个patch的信息。而不是增加信息的输入量，在与训练阶段，只有被mask掉的部分会进行梯度传播，而没mask的部分的输出是不会和token进行比较的。

所以，mask的东西，必须是attention的接受者。所以不能够mask一些节点的边。而是需要mask一些节点，让这些节点的邻居去学习这些节点自身的信息。

所以，我们延伸出两种预训练的方式。先回头看看我们的模型，当然，如果要做预训练相关的工作，也不止是局限于我们的模型，而对于我们的模型的优化，预训练的方式也只是可选择的方案之一。

1. input $X,A$
2. calculate A_k 等于距离每个人距离是k的点，如果一个点距离中心点的距离分别为i,j(i<j),该点仅在A_i出现
3. $neigbors_{n*neigbors*nhidden}={k=1...p}XW_k[Ai]$
4. attention(neigbors)
5. atteniton(XW_k,neigbors)
6. classifier
值得注意的细节有：
1. 在异性大的数据集中，模型对网络的层数敏感：如果在attention加全连接层，会导致效果显著变差，在同性大的数据集中没有明显差距

如果要加上预训练的机制，在此模型下，有两种考虑方式。
1. 最自然的想法是，在`5`阶段，mask对每个节点自身，mask掉其自身。用这个节点的邻域来学习这个节点本身的特征，来得到一个好的聚合方式。
2. 和beit的模仿可以考虑：在阶段`4`,mask部分节点，然后再通过中心节点的其它邻居来学习被mask的邻居。

还有两个指的注意的细节是：
1. 上述的方法`2`，有一个究极仿照bert的方案是，抛弃模型的阶段`5`，在上述模型进行`4`之后直接，拼在一起，作为该节点的低纬表征输出。整体相当于把这个节点的所有邻居当做一个句子去学习。当然，需要设计position_embeding保证节点的邻居地位的相同。这样一来，每一个节点的每一个邻居都可以直接的编码为一个数，就非常类似BEiT
2. 上面两种方案的使用并不相互矛盾

接下来产生的问题是:**如何学习一个可靠的低纬表征作为预训练的target**

## gae
两层gcn结构:

input: $X$，$A$

1. $H=GCN_1(X,A)$
2. $\mu =GCN_{2}^{\mu}(H,A),\sigma=GCN_{2}^{\sigma}(H,A)$
3. $Z=\mu +\epsilon *\sigma$
4. $A^{'}=sigmoid(ZZ^T)$
5. $loss=F(A,A^{'})$

在DGAT的实验存在错误，结果删除。

在GAT上重新进行实验。取得阶段性成功(83.5->84.7相同分割(跑一次))

## 一些细节

1. mask的方案：
    1. 所有节点为备选节点
    2. 在备选中随机选一个节点
    3. 把这个节点和这个节点的所有邻居都在备选点集中删除
    4. goto1
2. 并不存在收敛困难的问题，往往在800epochs，0.02lr的时候就会发生收敛
    
   


## 反思
不能使用邻居节点的任何特征去生成本节点的低维表达

这是因为，我们的目的是找到一个恰当的attention来计算我们的mask掉的特征值。如果我们用GCN去计算embeding会从理论上已经没办法学到一个漂亮的attention了。

之后我尝试朴素的VAE，但是loss-function可能需要改写
<image src='./data/9.001.png'>

GAT:

x->h -> l

x->h GAT

z
x

f -》 z -》f‘

1. gcn|GAT +mixhop +norm
2. 多跑数据 gnn benchmark
